---
title: "R Notebook"
output: html_notebook
---

# Packages

```{r setup, include=FALSE}
rm(list=ls())
pacman::p_load(tidyverse, dplyr, ggplot2, stringr, scales, lubridate, tidytext, tidyr, ggthemes, Rfacebook)
```



```{r}
# devtools::install_github("pablobarbera/Rfacebook/Rfacebook") # from GitHub
library(Rfacebook)
my_key <- "EAACEdEose0cBAA2OCJbYNSO0bzyegeuq3UK3f3ZCUy7miAThgRxaZAWspEhk3yAJ3NmlZCBLTB9r7MJlRCiTOGPXOOagU45XN6vCFygoPhd5ZC6WsMqnv045U3rQUyfcdKAUaLT9XqLITRLqMUX3GMJS9SXv0HgM62FScggr9e9NCtZA4ArfDSWb1woysfb4Qr827HAZByxwZDZD"
me <- getUsers("me", my_key, private_info=TRUE)
me


getUsers(c("barackobama", "donaldtrump"), my_key)
```

```{r}
milo <- "423006854503882"

page <- getPage(
  milo, 
  my_key, 
  n = 10000, 
  since='2016/12/07', 
  until='2017/12/07'
)

save(page, file = "data/page_milo.Rdata")
```

get Milo data

```{r}
post_ids <- page$id

get_post <- function(id){
  tryCatch({
    post <- getPost(
      id, 
      my_key, 
      n = 1000, 
      likes = T, 
      comments = T, 
      reactions = F
    )[[3]]
  }, error = function(e){
    post <- NA
  })

  cat(id, "\n")
  return(post)
}

post <- get_post(post_ids[1])
names(post)

post_list <- purrr::map(post_ids, get_post)
#save(post_list, file ="data/post_list.Rdata")
load("data/post_list.Rdata")
com_dat <- bind_rows(post_list)
glimpse(com_dat)
```


get Breitbart Data

```{r}
# ..f must be another functional that takes a .x and a .f
# ..f must have the usual purrr signature ..f(.x, .f, ...)
with_progress <- function(..x, ..f, .f, ...) {
  .f <- add_progress(.f, length(..x))
  ..f(..x, .f, ...)
}

breitbart <- "95475020353"

breitbart_page <- getPage(
  breitbart, 
  my_key, 
  n = 10000, 
  since='2016/12/07', 
  until='2017/12/07'
)

#save(breitbart_page, file = "breitbart_page.Rdata")
load("breitbart_page.Rdata")
pb <- progress_estimated(length(breitbart_page))

bb_dat <- breitbart_page$id[1:5] %>% 
  purrr::map_df(get_post, .pb=pb)
```


# text2vec

* [similarity](http://text2vec.org/similarity.html)

```{r}
library(stringr)
library(text2vec)

prep_fun <- function(x) {
  x %>% 
    # make text lower case
    str_to_lower %>% 
    # remove non-alphanumeric symbols
    str_replace_all("[^[:alnum:]]", " ") %>% 
    # collapse multiple spaces
    str_replace_all("\\s+", " ") %>%
    str_trim()
}

bb_dat$message <- prep_fun(bb_dat$message)
milo_dat <- com_dat %>%
  sample_n(size = 1000)

normalized <- function(x) (x-min(x))/(max(x)-min(x))


```

Iterator for tokens

```{r}
it1 <- itoken(bb_dat$message, progressbar = F)
it2 <- itoken(milo_dat$message, progressbar = F)
```


```{r}
it <- itoken(
  rbind(bb_dat, milo_dat)$message, 
  progressbar = F)

vocab <- create_vocabulary(it, ngram = c(2L, 2L)) %>%
  prune_vocabulary(
    doc_proportion_max = 0.1, 
    term_count_min = 5
  )

vectorizer <- vocab_vectorizer(vocab)
```



```{r}
dtm1 <- create_dtm(it1, vectorizer)
dim(dtm1)
```

```{r}
dtm2 <- create_dtm(it2, vectorizer)
dim(dtm2)
```


```{r}
d1_d2_jac_sim <- sim2(
  dtm1, dtm2, 
  #method = "jaccard",
  method = "cosine", 
  norm = "l2"
)

dim(d1_d2_jac_sim)

d1_d2_jac_sim[1:2, 1:5]
```


```{r}
tab <- as.matrix(d1_d2_jac_sim[1:50, 1:50])
gg1 <- tab %>%
  as.data.frame() %>%
  mutate(id = 1:n()) %>%
  gather("key", "value", -id) %>%
  ggplot(aes(x=id, y=key)) +
  geom_point(aes(colour=value), shape=15, size=10)

gg1 + viridis::scale_colour_viridis(direction = -1, option = "B")
```

```{r}
tab_colmeans <- d1_d2_jac_sim[1:50, 1:50] %>%
  as.matrix() %>%
  as.data.frame() %>% 
  mutate(corp1 = 1:n()) %>%
  gather("corp2", "value", -corp1) %>%
  group_by(corp2) %>%
  summarise(mvalue = sum(value)) %>%
  mutate(corp2 = as.numeric(corp2)) %>%
  arrange(corp2)
  
tab_colmeans %>%  
  ggplot(aes(mvalue)) +
  geom_histogram()

tab_colmeans %>%
  filter(mvalue > 0.0005) %>%
  summarise(mean(mvalue))
mean(tab_colmeans$mvalue)

ident_user <- tab_colmeans %>%
  filter(mvalue > 0)

bb_dat %>%
  mutate(id = 1:n()) %>%
  right_join(ident_user, by = c("id" = "corp2") )

  

```

## CNN

```{r}
library(stringr)
library(text2vec)

prep_fun <- function(x) {
  x %>% 
    # make text lower case
    str_to_lower %>% 
    # remove non-alphanumeric symbols
    str_replace_all("[^[:alnum:]]", " ") %>% 
    # collapse multiple spaces
    str_replace_all("\\s+", " ") %>%
    str_trim()
}
library(magrittr)
str_compare_docs <- function(text1, text2, n_gram = c(2L, 2L), average = T, ...){
  
  it1 <- text1 %>% 
    prep_fun() %>%
    itoken(progressbar = F)
  
  it2 <- text2 %>% 
    prep_fun() %>%
    itoken(progressbar = F)
  
  it <- itoken(c(text1, text2), progressbar = F)
  vocab <- create_vocabulary(it, ngram = n_gram) %>%
    prune_vocabulary(
      doc_proportion_max = 0.1, 
      term_count_min = 5
    )

  vectorizer <- vocab_vectorizer(vocab)
  dtm1 <- create_dtm(it1, vectorizer)
  dtm2 <- create_dtm(it2, vectorizer)
  
  d1_d2_jac_sim <- sim2(
    dtm1, dtm2, 
    #method = "jaccard",
    method = "cosine", 
    norm = "l2",
    ...
  ) 
  
  if(average){
    tab_colmeans <- d1_d2_jac_sim %>%
      as.matrix() %>%
      as.data.frame() %>% 
      mutate(corp1 = 1:n()) %>%
      gather("corp2", "value", -corp1) %>%
      group_by(corp2) %>%
      summarise(mvalue = mean(value)) %>%
      ungroup() %>%
      mutate(corp2 = as.numeric(corp2))
      #arrange(corp2)
      #filter(mvalue > 0) %>%
      #mutate(avg_cosin = normalized(avg_cosin))
  } else {
    d1_d2_jac_sim %<>%
      as.matrix() %>%
      as.data.frame() 
    return(d1_d2_jac_sim)
  }
  
  return(tab_colmeans)
}


text1 <- bb_dat$message[1:100]
text2 <- milo_dat$message[1:100]
nn <- str_compare_docs(text1, text2) 
nn
```



```{r}
cnn <- "5550296508"

cnn_page <- getPage(
  cnn, 
  my_key, 
  n = 10000, 
  since='2016/12/07', 
  until='2017/12/07'
)

#save(cnn_page, file = "data/cnn_page.Rdata")

cnn_dat <- cnn_page$id[1:5] %>% 
  purrr::map_df(get_post, .pb=pb)


n <- 1000
text1 <- cnn_dat$message[1:n]
text2 <- milo_dat$message[1:n]
gg_cnn <- str_compare_docs(text1, text2, average = F) %>%
  mutate(corp1 = 1:n()) %>%
  gather("corp2", "value", -corp1) %>%
  mutate(corp2 = as.numeric(corp2)) %>%
  arrange(corp1, corp2) %>%
  ggplot(aes(x=corp2, y=corp1)) +
  geom_point(aes(colour=value), shape=15, size=10) +
  viridis::scale_colour_viridis(direction = -1, option = "A") + theme(legend.position = "none")

cnn_hist <- str_compare_docs(text1, text2) %>%
  mutate(corp = "cnn")
  # filter(mvalue > 0) %>%
  # ggplot(aes(mvalue)) +
  # geom_histogram() +
  # ylim(0, 80)

text1 <- bb_dat$message[1:n]
text2 <- milo_dat$message[1:n]
gg_bb <- str_compare_docs(text1, text2, average = F) %>%
  mutate(corp1 = 1:n()) %>%
  gather("corp2", "value", -corp1) %>%
  mutate(corp2 = as.numeric(corp2)) %>%
  arrange(corp1, corp2) %>%
  ggplot(aes(x=corp2, y=corp1)) +
  geom_point(aes(colour=value), shape=15, size=10) +
  viridis::scale_colour_viridis(direction = -1, option = "A") + theme(legend.position = "none")

bb_hist <- str_compare_docs(text1, text2) %>%
  mutate(corp = "bb")


gg_hist <- rbind(cnn_hist, bb_hist) %>%
  arrange(corp2) %>%
  #group_by(corp) %>%
  #mutate(mvalue = scale(mvalue)) %>%
  ggplot(aes(x = mvalue, fill = corp, group=corp)) +
  geom_density(aes(y = ..density..), alpha = .5, color = NA, position = position_dodge())

library(gridExtra)
grid.arrange(gg_cnn, gg_bb, gg_hist, layout_matrix = matrix(c(1,3,2,3), ncol = 2))
```


# Sentiment

```{r}
trans_sentiment <- function(x){
  tab <- data.frame(x = x, stringsAsFactors = F) %>%
   mutate(id = 1:n()) %>%
   unnest_tokens(word, x) 

  bing <- tab %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "bing")
  
  nrc <- tab %>% 
    inner_join(get_sentiments("nrc")) %>% 
    filter(sentiment %in% c("positive", "negative")) %>%
    mutate(method = "nrc")
  
  afinn <- tab %>% 
    inner_join(get_sentiments("afinn")) %>% 
    mutate(sentiment = ifelse(score >0, "positive", "negative")) %>%
    mutate(method = "afinn", score = NULL)
  
  dat <- rbind(bing, nrc, afinn) %>%
    #mutate(sent = sentiment) %>%
    mutate(sent = ifelse(sentiment == "positive", 1, -1)) %>%
    group_by(id, method) %>%
    summarise(sent = mean(sent)) %>% 
    ungroup() %>%
    spread(key = "method", "sent")

  emotions <- tab %>% 
    inner_join(get_sentiments("nrc")) %>%
    rename(sent = sentiment) %>%
    group_by(id) %>%
    count(sent) %>%
    spread("sent", "n") %>%
    right_join(dat, by = "id")
    
  return(emotions)
}

cnn_sent <- trans_sentiment(x = cnn_dat$message[1:1000])


cnn_sent 
```

# Cognitive Complexity

```{r}
str_cci <- function(text, dict = NULL, scaling = F, normalizing = F, per_word = F, 
  all_stats = F) 
{
  disagg <- quanteda::dfm(text, dictionary = dict)
  disagg <- dplyr::tbl_df(quanteda::as.data.frame(disagg))
  words <- strsplit(text, "\\s+")
  disagg$nword <- sapply(words, length)
  disagg$SIXL <- sapply(words, function(x) length(which(nchar(x) > 
    5)))
  print("calculate ABE formula")
  cc_abe <- with(disagg, EXCL + TENT + NEGA + DISC + CONJ - 
    INCL)
  print("calculate OWENS formula")
  cc_owens <- with(disagg, DISC + SIXL + TENT + INCL + CAUS + 
    INSI + INHI - CERT - NEGA - EXCL)
  agg <- data.frame(cc_abe, cc_owens)
  normal <- function(x) {
    num <- x - min(x)
    denom <- max(x) - min(x)
    return(num/denom)
  }
  if (per_word) {
    agg$cc_abe_per_word <- as.numeric(agg$cc_abe/disagg$nword)
    agg$cc_owens_per_word <- as.numeric(agg$cc_owens/disagg$nword)
  }
  if (scaling) {
    agg$cc_abe_std <- scales::scale(agg$cc_abe)
    agg$cc_owens_std <- scales::scale(agg$cc_owens)
  }
  if (normalizing) {
    agg$cc_abe_norm <- normal(agg$cc_abe)
    agg$cc_owens_norm <- normal(agg$cc_owens)
  }
  if (all_stats) {
    agg <- list(indicators = disagg, outputs = agg)
  }
  print("done!")
  return(agg)
}


cc_dict_eng <- read.csv(
  "ccdict_eng.csv", 
  sep = ",", dec = ".", 
  header = T, stringsAsFactors = F, 
  encoding = "UTF-8", as.is = T)

cc_dict_eng$word %<>% 
  str_trim()

### build dict list
dict_eng <- list(
  "COGM" = cc_dict_eng$word[cc_dict_eng$category %in% "COGM"],
  "INHI" = cc_dict_eng$word[cc_dict_eng$category %in% "INHI"],
  "DISC" = cc_dict_eng$word[cc_dict_eng$category %in% "DISC"],
  "TENT" = cc_dict_eng$word[cc_dict_eng$category %in% "TENT"],
  "INCL" = cc_dict_eng$word[cc_dict_eng$category %in% "INCL"],
  "CAUS" = cc_dict_eng$word[cc_dict_eng$category %in% "CAUS"],
  "INSI" = cc_dict_eng$word[cc_dict_eng$category %in% "INSI"],
  "CERT" = cc_dict_eng$word[cc_dict_eng$category %in% "CERT"],
  "NEGA" = cc_dict_eng$word[cc_dict_eng$category %in% "NEGA"],
  "EXCL" = cc_dict_eng$word[cc_dict_eng$category %in% "EXCL"],
  "CONJ" = cc_dict_eng$word[cc_dict_eng$category %in% "CONJ"]
)



library(quanteda)
dict_cc_eng <- dictionary(dict_eng, format = "LIWC", encoding = "UTF-8")
save(dict_cc_eng, file="dict_cc_eng")



str_cci(cnn_dat$message[1:10], dict = dict_mf_eng)
```


# Moral Foundations

```{r}

dict_mf_eng <- dictionary(
  file = "http://www.moralfoundations.org/sites/default/files/files/downloads/moral%20foundations%20dictionary.dic"
  , format = "LIWC")

library(quanteda)

load("dict_mf_eng")
m <- quanteda::dfm(cnn_dat$message[1:10], dictionary = dict_mf_eng) %>%
  as.matrix() %>%
  as.data.frame()


require(quanteda)
mfdict <- dictionary(file = "http://www.moralfoundations.org/sites/default/files/files/downloads/moral%20foundations%20dictionary.dic", format = "LIWC")
dfm(cnn_dat$message[1:10], dictionary = mfdict)

### Care/harm
# Fürsorglichkeit, Höfflichkeit
# kindness, gentleness, and nurturance
corpus_mf$HarmVirtue #
corpus_mf$HarmVice

### Fairness/cheating
# reciprocal altruism
corpus_mf$FairnessVirtue
corpus_mf$FairnessVice

### Loyalty/betrayal
# history as tribal creatures able to form shifting coalitions
# patriotism and self-sacrifice for the group
# "one for all, and all for one."
corpus_mf$IngroupVirtue
corpus_mf$IngroupVice

### Authority/subversion
# hierarchical social interactions. It underlies virtues of 
# leadership and followership, including deference to legitimate 
# authority and respect for traditions.
corpus_mf$AuthorityVirtue
corpus_mf$AuthorityVice

### Sanctity/degradation
# psychology of disgust and contamination
# immoral activities and contaminants 
# an idea not unique to religious traditions.
corpus_mf$PurityVirtue
corpus_mf$PurityVice

### Liberty/oppression
# reactance and resentment people feel toward those who 
# dominate them and restrict their liberty
corpus_mf$MoralityGeneral


### save mf
setwd(data)
save(corpus_mf, file="corpus_mf.Rdata")


```


# Alt Right Corpus

```{r}
altright_list <- list(
  "milo" = "423006854503882",
  "coulter" = "695526053890545",
  "cernovich" = "427302110732180",
  "alexjones" = "6499393458",
  "PJW" = "242174545810040",
  "lauren" = "184795298567879",
  "roaming" = "1213352768743538",
  "bps" = "359762154043841",
  "blairewhite" = "1136054926414603",
  "disdain" = "1772552276096911",
  "foxnews" = "15704546335",
  "msnbc" = "273864989376427",
  "breitbart" = "95475020353",
  "cnn" = "5550296508",
  "tucker" = "368557930146199",
  "hannity" = "69813760388",
  "wapo" = "6250307292",
  "djt" = "153080620724",
  "nyt" = "5281959998",
  "abc" = "86680728811",
  "infowars" = "80256732576"
)

my_key <- "EAACEdEose0cBAJMD6gwuYrZC7i2YG9Tih34kQE7LK20nZCWsoFB0z9WLjLZAYxNqshUMiCZBn25EEU3LhSV1QufxyMb7CUAOwEz0AeP1rDeacuZAlmyFjKBjGZC8rn9MmFOLM16n0ZCM1RK64cND3yZBiovhjBj85wbnZBCo7CE7ABn7vHZC2ool8M09X9JVG4kt5nQookuOU9JAZDZD"


#data_list <- list()
for(jj in 13:length(altright_list)){
  
  id <- altright_list[[jj]]
  
  page <- getPage(
    id, 
    my_key, 
    n = 10000, 
    since='2016/12/07', 
    until='2017/12/07'
  )
  
  save(page, file = paste0("data/", names(altright_list)[jj], ".Rdata"))
  
  comments <- list()
  for(ii in seq_along(page$id)){
    comments[[ii]] <- get_post(page$id[ii])
    cat(ii)
  }
  comments <- bind_rows(comments)
  #comments <- page$id %>% 
  #  purrr::map_df(get_post)
  
  save(comments, file = paste0("data/",names(altright_list)[jj], "_comments.Rdata"))
  
  cat(jj, "\n")
  data_list[[jj]] <- comments
}

comments <- bind_rows(data_list)
```

```{r}
dir("data/")[dir("data/") %>% str_detect("_comments")]

arnames <- c("alexjones", "blairewhite", "bps", "cernovich", "coulter", "lauren", "infowars", "lauren", "milo", "PJW")
alt_trolls <- paste0("data/", arnames, "_comments.Rdata")
#media_data <- alt_trolls %>% purrr::map_df(load)

dat <- get(load(alt_trolls[1])) %>%
    mutate(origin = arnames[1])
for(jj in 2:length(alt_trolls)){
  temp <- get(load(alt_trolls[jj])) 
  temp <- temp %>%
    mutate(origin = arnames[jj])
  dat <- rbind(dat, temp) 
  cat(jj)
}

comments <- dat
rm(dat)
object.size(comments)
save(comments, file = "coments_all.Rdata")
```

## Chorpus validation

```{r}
load("coments_all.Rdata")
glimpse(comments)

comments_sent <- trans_sentiment(x = comments$message[1:100])
glimpse(comments_sent)

comments %>%
  mutate(id = 1:n()) %>%
  left_join(comments_sent) %>%
  filter(anger > 1.5)
```


```{r}
library(text2vec)
library(LDAvis)
tokens <- comments$message[1:1000] %>% 
  str_to_lower() 

it <- text2vec::itoken(iterable = comments$message[1:100], progressbar = FALSE)
v = create_vocabulary(it) %>% 
  prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.2)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")

lda_model = LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr = 
  lda_model$fit_transform(x = dtm, n_iter = 1000, 
                          convergence_tol = 0.001, n_check_convergence = 25, 
                          progressbar = FALSE)

lda_model$plot()
```


















```{r}
library(text2vec)
load("coments_all.Rdata")
msnbc <- get(load("data/msnbc_comments.Rdata"))
fox <- get(load("data/foxnews_comments.Rdata"))

n <- 1000
gg_msnbc <- str_compare_docs(comments$message[1:n], msnbc$message[1:n], average = F) %>%
  mutate(corp1 = 1:n()) %>%
  gather("corp2", "value", -corp1) %>%
  mutate(corp2 = as.numeric(corp2)) %>%
  arrange(corp1, corp2) %>%
  ggplot(aes(x=corp2, y=corp1)) +
  geom_point(aes(colour=value), shape=15, size=10) +
  viridis::scale_colour_viridis(direction = -1, option = "A") + theme(legend.position = "none")

msnbc_hist <- str_compare_docs(comments$message[1:n], msnbc$message[1:n]) %>%
  mutate(corp = "msnbc")

gg_fox <- str_compare_docs(comments$message[1:n], fox$message[1:n], average = F) %>%
  mutate(corp1 = 1:n()) %>%
  gather("corp2", "value", -corp1) %>%
  mutate(corp2 = as.numeric(corp2)) %>%
  arrange(corp1, corp2) %>%
  ggplot(aes(x=corp2, y=corp1)) +
  geom_point(aes(colour=value), shape=15, size=10) +
  viridis::scale_colour_viridis(direction = -1, option = "A") + theme(legend.position = "none")

fox_hist <- str_compare_docs(comments$message[1:n], fox$message[1:n]) %>%
  mutate(corp = "fox")


gg_hist <- rbind(msnbc_hist, fox_hist) %>%
  arrange(corp2) %>%
  #group_by(corp) %>%
  #mutate(mvalue = scale(mvalue)) %>%
  ggplot(aes(x = mvalue, fill = corp, group=corp)) +
  geom_density(aes(y = ..density..), alpha = .5, color = NA, position = position_dodge())

library(gridExtra)
grid.arrange(gg_msnbc, gg_fox, gg_hist, layout_matrix = matrix(c(1,3,2,3), ncol = 2))
```


```{r}
fox_dat <- str_compare_docs(comments$message[1:10000], fox$message[1:n]) 
fox_dat$id <- 1:n
fox_dat %>%
  right_join(fox$message[1:n]
cnn_sent <- trans_sentiment(x = cnn_dat$message[1:1000])

```

