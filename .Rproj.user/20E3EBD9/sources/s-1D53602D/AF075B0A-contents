---
title: "NLP against radicalization"
subtitle: "Corpus validation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Packages

```{r setup, include=FALSE}
rm(list=ls())
pacman::p_load(tidyverse, dplyr, ggplot2, stringr, scales, lubridate, tidytext, tidyr, ggthemes, textstem, Rfacebook)
```

# Data 

```{r}
load("coments_all.Rdata")
glimpse(comments)


table(str_replace(comments$origin, pattern = "_.*?$", ""))
```

# Prepare Data

```{r}
str_clean <- function(x, new_stop = NULL){
  
  sw <- tm::stopwords("eng")
  if (!is.null(new_stop)) sw <- append(sw, new_stop)
  
  library(dplyr)
  text_dat <- data.frame(id = 1:length(x), ctext = x, stringsAsFactors = F) %>%
    mutate(block = ntile(ctext, 20))
  
  stopwords <- function(x) {
    x %>%
      tidytext::unnest_tokens(word, ctext) %>%
      anti_join(data.frame(word = sw)) %>%
      group_by(id) %>%
      summarise(comment = paste(word, collapse = " ")) %>%
      mutate(comment = 
        comment %>% 
          stringr::str_replace_all(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", "") %>%
          textstem::lemmatize_strings() %>%
          stringr::str_replace_all("\\'", "") %>%
          stringr::str_replace_all("[^a-zA-Zäüöß\\s]", " ") %>%
          stringr::str_to_lower() %>% 
          stringr::str_trim()
      )
    cat("block:", x$block[1], "\n")
  }
  
  # cvector <- list()
  # for (jj in seq_along(unique(text_dat$block))) {
  #   cvector[[jj]] <- stopwords(x = text_dat[text_dat$block == jj,]) 
  #   cat(jj, "\n")
  # }
  
  text_dat %>%
    split(.$block) %>% # from base R
    map(~ stopwords(x =.)) %>%
    bind_rows()
  
  cat("... FINISHED ...")
  return(text_dat)
}
```






```{r}
#library(textstem)
#library(systats)
### clean shit of
comments_smp <- comments %>%
  filter(message != "")# %>%
  #slice(1:100000)
  #sample_n(size = 10000)

words <- comments_smp$message %>%
  str_split("\\s+") 
comments_smp$nword <- sapply(words, length)

comments_smp <- comments_smp %>%
  filter(nword > 10)

comments_smp$ctext <- str_clean(x = comments_smp$message)

library(magrittr)
#comments_smp %<>%
#  drop_na(ctext)

#save(comments_smp, file ="comments_smp.Rdata")
load("comments_smp.Rdata")
```

# Sentiment Analysis

```{r}
trans_sentiment <- function(x){
  tab <- data.frame(x = x, stringsAsFactors = F) %>%
   mutate(id = 1:n()) %>%
   unnest_tokens(word, x) 

  bing <- tab %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "bing")
  
  nrc <- tab %>% 
    inner_join(get_sentiments("nrc")) %>% 
    filter(sentiment %in% c("positive", "negative")) %>%
    mutate(method = "nrc")
  
  afinn <- tab %>% 
    inner_join(get_sentiments("afinn")) %>% 
    mutate(sentiment = ifelse(score > 0, "positive", "negative")) %>%
    mutate(method = "afinn", score = NULL)
  
  dat <- rbind(bing, nrc, afinn) %>%
    #mutate(sent = sentiment) %>%
    mutate(sent = ifelse(sentiment == "positive", 1, -1)) %>%
    group_by(id, method) %>%
    summarise(sent = mean(sent)) %>% 
    ungroup() %>%
    spread(key = "method", "sent")

  emotions <- tab %>% 
    inner_join(get_sentiments("nrc")) %>%
    rename(sent = sentiment) %>%
    group_by(id) %>%
    count(sent) %>%
    spread("sent", "n") %>%
    right_join(dat, by = "id")
    
  return(emotions)
}

comments_sent <- trans_sentiment(x = comments_smp$message)


comments1 <- comments_smp %>%
  mutate(id = 1:n()) %>%
  left_join(comments_sent)

# save(comments1, file = "comments_final100-23.Rdata")

comments1 %<>% filter(anger > 0)
```


# LDA

```{r}
library(text2vec)
library(LDAvis)

it <- text2vec::itoken(iterable = comments1$ctext, progressbar = FALSE)
vocab <- create_vocabulary(it) %>% 
  prune_vocabulary(term_count_min = 1, doc_proportion_max = 0.2)
vectorizer <- vocab_vectorizer(vocab)
#dtm <- create_dtm(it, vectorizer) # , type = "dgTMatrix"

library(tidyr)
#coommets_topics <- 
```


## word2vec

```{r}
#devtools::install_github("dselivanov/text2vec")
library(text2vec)
library(LDAvis)
dtm <- create_dtm(it, vectorizer, type = "dgTMatrix") 

lda_model <- text2vec::LDA$new(
  n_topics = 10, 
  doc_topic_prior = 0.1, 
  topic_word_prior = 0.01
)

doc_topic_distr <- lda_model %>% 
  fit_transform(
    x = dtm, n_iter = 1000, 
    convergence_tol = 0.001, 
    n_check_convergence = 25, 
    progressbar = F
  )

lda_model$plot()
lda_model$get_top_words(n = 10, topic_number = 2, lambda = .2)
```



## topicmodels

```{r}
library(topicmodels)
dtm <- create_dtm(it, vectorizer)
comments_lda <- topicmodels::LDA(dtm, k = 20, control = list(seed = 2017))
```


## Exploring

```{r}
glimpse(comments_smp)

marx_cleaned <- comments_smp$ctext$ctext %>% 
  str_detect(pattern = "cultural marxism")

marx_comms <- comments_smp$ctext$ctext[marx_cleaned]

marx_comms[1:25]


white_cleaned <- comments_smp$ctext$ctext %>% 
  str_detect(pattern = "white genocide")

white_comms <- comments_smp$ctext$ctext[white_cleaned]

white_comms[1:25]
```

